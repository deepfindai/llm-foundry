max_seq_len: 1024
seed: 1
precision: amp_fp16


models:
-
  model_name: text-babbage-001
  model:
    name: openai
  tokenizer:
    name: openai
    kwargs:
      name: text-babbage-001

device_eval_batch_size: 4

# FSDP config for model sharding
fsdp_config:
  sharding_strategy: FULL_SHARD
  mixed_precision: FULL

icl_tasks: 'eval/yamls/tasks_light.yaml'
model_gauntlet: 'eval/yamls/model_gauntlet.yaml'
